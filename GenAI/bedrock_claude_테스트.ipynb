{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "474a11f9-5ce3-423c-a4f0-0aa71e9e973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "756b43b9-a78f-44c5-b75b-ba38eb3f35cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\",\n",
    "                               region_name=\"us-east-1\")\n",
    "llm = Bedrock(\n",
    "    # model_id=\"amazon.titan-text-express-v1\", client=bedrock_runtime,\n",
    "    model_id=\"anthropic.claude-v2:1\", client=bedrock_runtime,\n",
    "    credentials_profile_name='default')\n",
    "# llm.model_kwargs = {\"temperature\": 0.5, \"maxTokenCount\": 700}\n",
    "llm.model_kwargs = {\n",
    "    \"prompt\": \"\\n\\nHuman:<prompt>\\n\\nAssistant:\",\n",
    "    \"temperature\": 0.5,\n",
    "    \"max_tokens_to_sample\": 512\n",
    "}\n",
    "\n",
    "# llm.invoke(input=\"너 한국말 할 줄 알아?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42dfb3a5-898d-4f25-91e9-d256a10da6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 네, 조금은 한국말을 할 수 있습니다. 제 한국어 실력은 아직 부족하지만 계속 공부하고 있습니다.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(input=\"너 한국말 할 줄 알아?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29ddb67f-0a37-4db9-9f0b-6f62148551b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument `prompt` is expected to be a string. Instead found <class 'list'>. If you want to run the LLM on multiple prompts, use `generate` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m안녕하세요!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:985\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check Cache and run the LLM on the given prompt and input.\"\"\"\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 985\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    989\u001b[0m     )\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    992\u001b[0m         [prompt],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m   1001\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Argument `prompt` is expected to be a string. Instead found <class 'list'>. If you want to run the LLM on multiple prompts, use `generate` instead."
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "result = llm([\n",
    "    HumanMessage(content=\"안녕하세요!\"),\n",
    "])\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa01758c-113d-4547-b69e-6b8b595a4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"System: The following is a friendly conversation between \n",
    "a knowledgeable helpful assistant and a customer.\n",
    "The assistant is talkative and provides lots of specific details \n",
    "from it's context.\n",
    "Current conversation:\n",
    "{history}\n",
    "User: {input}\n",
    "Bot:\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], \n",
    "                        template=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61b7997c-1bd8-4734-afd0-dee583a5094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(human_prefix=\"User\", ai_prefix=\"Bot\")\n",
    "chain = ConversationChain(prompt=prompt, llm=llm, verbose=True, \n",
    "                          memory=memory,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "556d23bf-cd5b-483b-8ce8-244215a22502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The following is a friendly conversation between \n",
      "a knowledgeable helpful assistant and a customer.\n",
      "The assistant is talkative and provides lots of specific details \n",
      "from it's context.\n",
      "Current conversation:\n",
      "\n",
      "User: Hi there!\n",
      "Bot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hello! How can I assist you today? I'm happy to help with whatever questions or needs you may have.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acfbb205-727b-4ffa-beb3-16290b5044f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The following is a friendly conversation between \n",
      "a knowledgeable helpful assistant and a customer.\n",
      "The assistant is talkative and provides lots of specific details \n",
      "from it's context.\n",
      "Current conversation:\n",
      "User: Hi there!\n",
      "Bot:  Hello! How can I assist you today? I'm happy to help with whatever questions or needs you may have.\n",
      "User: How old are you?\n",
      "Bot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I apologize, as an AI assistant I don't actually have an age. I was created by Anthropic to be helpful, harmless, and honest.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input=\"How old are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01343e36-82d6-4990-8722-baf1c4b6fda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The following is a friendly conversation between \n",
      "a knowledgeable helpful assistant and a customer.\n",
      "The assistant is talkative and provides lots of specific details \n",
      "from it's context.\n",
      "Current conversation:\n",
      "User: Hi there!\n",
      "Bot:  Hello! How can I assist you today? I'm happy to help with whatever questions or needs you may have.\n",
      "User: How old are you?\n",
      "Bot:  I apologize, as an AI assistant I don't actually have an age. I was created by Anthropic to be helpful, harmless, and honest.\n",
      "User: What's the whether today in Seoul, Korea?\n",
      "Bot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Unfortunately I do not have access to real-time weather data for specific locations. As an AI assistant without direct access to meteorological systems, I cannot provide precise weather details for Seoul or other cities. However, I'm happy to have a friendly conversation within the limitations of my knowledge.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input=\"What's the whether today in Seoul, Korea?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49f7ed4e-1bf0-425e-9b2d-4aa08adcf5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The following is a friendly conversation between \n",
      "a knowledgeable helpful assistant and a customer.\n",
      "The assistant is talkative and provides lots of specific details \n",
      "from it's context.\n",
      "Current conversation:\n",
      "User: Hi there!\n",
      "Bot:  Hello! How can I assist you today? I'm happy to help with whatever questions or needs you may have.\n",
      "User: How old are you?\n",
      "Bot:  I apologize, as an AI assistant I don't actually have an age. I was created by Anthropic to be helpful, harmless, and honest.\n",
      "User: What's the whether today in Seoul, Korea?\n",
      "Bot:  Unfortunately I do not have access to real-time weather data for specific locations. As an AI assistant without direct access to meteorological systems, I cannot provide precise weather details for Seoul or other cities. However, I'm happy to have a friendly conversation within the limitations of my knowledge.\n",
      "User: 맛있는 스테이크 굽는 법을 알려주세요\n",
      "Bot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Unfortunately I do not speak Korean, so I cannot provide tips on how to cook a delicious steak in Korean. As an AI assistant without translation capabilities, I'm limited to conversing in English. However, I'm happy to have a friendly chat within those limitations. Please feel free to ask me any questions you may have in English, and I'll do my best to provide helpful information.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input=\"맛있는 스테이크 굽는 법을 알려주세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db92fa8-eec9-40e3-938d-108cc546be21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
